{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from ml_lib.models.pla import PerceptronLearningAlgorithm\n",
    "%matplotlib qt\n",
    "rng = np.random.default_rng(seed=21)  # Set the seed for the random generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_size = (14, 10)\n",
    "RED = \"#E57873\" #Red\n",
    "BLUE = \"#94CAE0\" #Blue\n",
    "ORANGE = \"#FF9D26\" #Orange\n",
    "PRETTY_CMAP = ListedColormap([BLUE, RED, ORANGE])\n",
    "\n",
    "def get_spiral_data(num_points=100, num_classes=3, num_features=2, save=False):\n",
    "    N = num_points # number of points per class\n",
    "    K = num_classes # number of classes\n",
    "    D = num_features  # dimensionality\n",
    "\n",
    "    X = np.zeros((N*K, D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "    for j in range(K):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        r = np.linspace(0.0,1,N) # radius\n",
    "        t = np.linspace(j*4,(j+1)*4, N) + rng.integers(0, N)*0.2 # theta\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "        \n",
    "    # lets visualize the data:\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=80, cmap=PRETTY_CMAP, edgecolors='k')\n",
    "    # equal axis\n",
    "    plt.axis('equal')\n",
    "    plt.title('Spiral Data')\n",
    "    plt.show()\n",
    "    return X, y\n",
    "\n",
    "# Generate a 2D classification dataset perfectly seperated by a line\n",
    "def get_linear_data(n_points=100, save_fig = False, margin=1.0):\n",
    "    '''\n",
    "    Generate a 2D classification dataset perfectly seperated by a line\n",
    "\n",
    "    :param n_points: the number of data points\n",
    "    :param save_fig: whether to save the figure\n",
    "    :param margin: the margin of the classifier\n",
    "    :return x: x values [n_points x 2]\n",
    "    :return y: y values [n_points x 1]\n",
    "    :return w: the normal vector of the line\n",
    "    :return b: the bias term of the line\n",
    "    '''\n",
    "    # Generate n_points between [-10, 10]\n",
    "    data_range = np.array([-10, 10])\n",
    "\n",
    "    # Generate a random line y = mx + c where m is the slope and c is the y-intercept\n",
    "    w = rng.random(2) # Randomly generate the slope and y-intercept\n",
    "    b = rng.random(1)\n",
    "    \n",
    "    i_points = 0\n",
    "    x = np.empty((n_points, 2))\n",
    "    y = np.empty((n_points, 1))\n",
    "\n",
    "    while(i_points < n_points):\n",
    "        x1 = rng.uniform(*data_range)\n",
    "        x2 = rng.uniform(*data_range)\n",
    "        x[i_points] = [x1, x2]\n",
    "\n",
    "        # Assign labels based on the line if they perfectly lie on one side of the line\n",
    "        distance = distance_from_line((x1, x2), w, b)\n",
    "        if abs(distance) < margin:\n",
    "            continue\n",
    "        else:\n",
    "            y_i = np.sign(w.dot(x[i_points].T)-b)\n",
    "            y[i_points] = y_i\n",
    "            i_points += 1\n",
    "    return x, y, w, b\n",
    "\n",
    "def plot_data(x, y, fig = None):\n",
    "    # Plot the data\n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=fig_size)\n",
    "    else:\n",
    "        plt.figure(fig.number)\n",
    "    \n",
    "    # Label both the data points and the line\n",
    "    labels = ['Class 1', 'Class 2', 'Line']\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, s=100, cmap=PRETTY_CMAP, edgecolors='k')\n",
    "    plt.legend(labels)\n",
    "    plt.axis('equal')\n",
    "    plt.title('Linearly Separable Data')\n",
    "    return fig\n",
    "\n",
    "def plot_decision_boundary(x, y, w, b, label=None, fig = None, alpha=0.5, label_line=True):\n",
    "\n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=fig_size)\n",
    "    else:\n",
    "        plt.figure(fig.number)\n",
    "\n",
    "    # Calculate the decision boundary\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "    x1_boundary = x1\n",
    "    x2_boundary = (-w[0]*x1_boundary + b)/w[1]\n",
    "    \n",
    "    # Calculate the center of the line\n",
    "    x1_center = (np.max(x1_boundary) + np.min(x1_boundary))/2\n",
    "    x2_center = (np.max(x2_boundary) + np.min(x2_boundary))/2\n",
    "    \n",
    "    # normalize the normal vector\n",
    "    w_normed = 5*w/np.linalg.norm(w)\n",
    "\n",
    "    # Plot the decision boundary and the normal vector    \n",
    "    c = RED\n",
    "    if label == 'True Decision Boundary':\n",
    "        c = BLUE\n",
    "\n",
    "    plt.plot(x1_boundary, x2_boundary, c=c, label=label, linewidth=2, linestyle='--', alpha=alpha)\n",
    "    plt.quiver(x1_center, x2_center, w_normed[0], w_normed[1], angles='xy', scale_units='xy', scale=1, color=c, alpha=alpha)\n",
    "\n",
    "    # Calculate the margin of the classifier\n",
    "    min_margin, _ = compute_margin(x, y, w, b)\n",
    "    mistake_bound_val = mistake_bound(x, min_margin)\n",
    "    misclassifications = misclassified_points(x, y, w, b)\n",
    "    plt.legend()\n",
    "    plt.title('Linearly Separable Data with Margin: {:.2f}, Mistake Bound: {:.2f}, Misclassifications: {:.1f}'.format(min_margin, mistake_bound_val, misclassifications))\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    xlim = np.array([np.min(x1), np.max(x1)])\n",
    "    ylim = np.array([np.min(x2), np.max(x2)])\n",
    "\n",
    "    plt.xlim(1.1*xlim)\n",
    "    plt.ylim(1.1*ylim)\n",
    "    return fig\n",
    "\n",
    "def distance_from_line(x, w, b):\n",
    "    \"\"\"\n",
    "    Compute the distance of a point from a line\n",
    "    x: a point\n",
    "    w: the normal vector of the line\n",
    "    b: the bias term of the line\n",
    "    \"\"\"\n",
    "    distance = (w.T.dot(x) - b)/np.linalg.norm(w)\n",
    "    return distance\n",
    "\n",
    "def misclassified_points(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the number of misclassified points\n",
    "    x: the data points\n",
    "    y: the labels\n",
    "    w: the normal vector of the line\n",
    "    b: the bias term of the line\n",
    "    \"\"\"\n",
    "    misclassified_points = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        if y[i] != np.sign(np.dot(x[i], w) - b):\n",
    "            misclassified_points += 1\n",
    "    return misclassified_points\n",
    "\n",
    "# Margin of the classifier is calculated as the maximum distance of the misclassified points from the line \n",
    "def compute_margin(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the margin of a point from a line\n",
    "    x: a point\n",
    "    w: the normal vector of the line\n",
    "    b: the bias term of the line\n",
    "    \"\"\"\n",
    "    margin = y.T*(w.dot(x.T))/np.linalg.norm(w)\n",
    "\n",
    "    # Find the minimum margin and the point that has the minimum margin\n",
    "    min_margin = np.min(margin)\n",
    "    min_margin_point = x[np.argmin(margin)]\n",
    "    return min_margin, min_margin_point\n",
    "\n",
    "def mistake_bound(x, margin):\n",
    "    \"\"\"\n",
    "    Maximum number of mistakes perceptron algorithm could make on a linearly separable dataset\n",
    "    https://svivek.com/teaching/lectures/slides/perceptron/perceptron-mistake-bound.pdf\n",
    "    n_features: the number of features\n",
    "    margin: the margin of the classifier\n",
    "    \"\"\"\n",
    "    R = np.max(np.linalg.norm(x, axis=1))\n",
    "    return (R**2)/(margin**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified points:  15  Margin:  -4.549562690205908\n",
      "Number of misclassified points:  15  Margin:  -4.549562690205908\n",
      "Number of misclassified points:  15  Margin:  -4.549562690205908\n",
      "Number of misclassified points:  15  Margin:  -4.549562690205908\n",
      "Updated weights: [0.33316542 0.47495883], bias: [0.56458904]\n",
      "Number of misclassified points:  10  Margin:  -3.051705530944294\n",
      "Number of misclassified points:  10  Margin:  -3.051705530944294\n",
      "Number of misclassified points:  10  Margin:  -3.051705530944294\n",
      "Number of misclassified points:  10  Margin:  -3.051705530944294\n",
      "Number of misclassified points:  10  Margin:  -3.051705530944294\n",
      "Number of misclassified points:  10  Margin:  -3.051705530944294\n",
      "Updated weights: [0.36438289 0.46301921], bias: [0.55458904]\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Number of misclassified points:  8  Margin:  -2.4084170408166177\n",
      "Updated weights: [0.44190228 0.37136592], bias: [0.56458904]\n",
      "Number of misclassified points:  0  Margin:  0.03795460343449965\n"
     ]
    }
   ],
   "source": [
    "# Get the data and the true line\n",
    "x, y, w, b = get_linear_data(margin=1.0)\n",
    "\n",
    "# Plot the data\n",
    "fig = plot_data(x, y)\n",
    "\n",
    "# Initialize the Perceptron Learning Algorithm\n",
    "pla = PerceptronLearningAlgorithm(learning_rate=0.01, rng=rng)\n",
    "\n",
    "# set misclassified point to inf\n",
    "misclassified = misclassified_points(x, y, pla.weights, pla.bias)\n",
    "last_w = pla.weights\n",
    "last_b = pla.bias\n",
    "iteration = 0\n",
    "while misclassified > 0:\n",
    "    # Clear the display and plot the data and the true line\n",
    "    plt.clf()\n",
    "    plot_data(x, y, fig)\n",
    "    plot_decision_boundary(x, y, w, b, 'True Decision Boundary', fig)\n",
    "    \n",
    "    # Get random data point\n",
    "    rnd_idx = rng.integers(0, x.shape[0])\n",
    "    x_i = x[rnd_idx].reshape(1, 2)\n",
    "    y_i = y[rnd_idx]\n",
    "    \n",
    "    # Perform prediction on the current to get the current decision boundary\n",
    "    y_pred_i = pla.predict(x_i)\n",
    "    \n",
    "    # Mark the point that was used for training based on the prediction\n",
    "    edge_color = 'g'\n",
    "    if y_pred_i != y_i:\n",
    "        edge_color = 'r'\n",
    "\n",
    "    # Plot the last decision boundary    \n",
    "    last_w = pla.weights\n",
    "    last_b = pla.bias\n",
    "    plot_decision_boundary(x, y, last_w, last_b, 'Last Decision Boundary', fig=fig, alpha=0.2)\n",
    "\n",
    "    # Mark the point that was used for training\n",
    "    plt.scatter(x_i[0, 0], x_i[0, 1], s=140, edgecolors=edge_color, facecolors='none', linewidth=3, label='Training Point')\n",
    "\n",
    "    # Update the model based on the current point.\n",
    "    # Model is updated only if the prediction is incorrect\n",
    "    pla.train(x_i, y_i)\n",
    "\n",
    "    # Plot the new decision boundary\n",
    "    plot_decision_boundary(x, y, pla.weights, pla.bias, 'Predicted Decision Boundary', fig=fig, alpha=0.75)\n",
    "    \n",
    "    # Calculate the margin of the classifier\n",
    "    # y_pred = pla.predict(x)\n",
    "    margin, min_margin_pt = compute_margin(x, y, pla.weights, pla.bias)\n",
    "\n",
    "    # Mark the point that has the minimum margin\n",
    "    plt.scatter(min_margin_pt[0], min_margin_pt[1], s=140, edgecolors='b', facecolors='none', linewidth=3, label='Margin Point')\n",
    "    plt.legend()\n",
    "    \n",
    "    # get parent working directory\n",
    "    pwd = os.path.dirname(os.getcwd())\n",
    "\n",
    "    # get the repository path not the cwd\n",
    "    result_path = pwd + \"/results/bias_variance_tradeoff/\"\n",
    "    # file = str(iteration) + 'pla.png'\n",
    "    # iteration += 1\n",
    "    # plt.savefig(file, dpi=600)\n",
    "    \n",
    "    # Calculate the number of misclassified points\n",
    "    misclassified = misclassified_points(x, y, pla.weights, pla.bias)\n",
    "    plt.pause(0.001)\n",
    "    plt.waitforbuttonpress()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
